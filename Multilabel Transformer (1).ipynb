{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9152e6f4-66f8-44e5-8060-8f51b39d7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Function to create dataframe from jsonl file\n",
    "def create_dataframe_from_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            data.append({'paragraph': record['paragraph'], 'labels': record['labels']})\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "train_path = 'ArMPro_multilabel_train.jsonl'\n",
    "dev_path = 'ArMPro_multilabel_dev.jsonl'\n",
    "test_path = 'ArMPro_multilabel_test.jsonl'\n",
    "\n",
    "train = create_dataframe_from_jsonl(train_path)\n",
    "dev = create_dataframe_from_jsonl(dev_path)\n",
    "test = create_dataframe_from_jsonl(test_path)\n",
    "\n",
    "# Reading labels \n",
    "with open('persuasion_techniques_list.txt', 'r') as file:\n",
    "    all_labels = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Convert labels to binary \n",
    "mlb = MultiLabelBinarizer(classes=all_labels)\n",
    "train['binary_labels'] = list(mlb.fit_transform(train['labels']))\n",
    "dev['binary_labels'] = list(mlb.transform(dev['labels']))\n",
    "test['binary_labels'] = list(mlb.transform(test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e384ba7-2b1c-4c5f-8c9e-944b7676daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alharbi, Alaa, and Mark Lee. \"Kawarith: an Arabic Twitter Corpus for Crisis Events.\"\n",
    "# Proceedings of the Sixth Arabic Natural Language Processing Workshop. 2021\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/alaa-a-a/multi-dialect-arabic-stop-words/main/Stop-words/stop_list_1177.txt\n",
    "arabic_stop_words = []\n",
    "with open ('./stop_list_1177.txt',encoding='utf-8') as f :\n",
    "    for word in f.readlines() :\n",
    "        arabic_stop_words.append(word.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2220c52a-7778-4544-a741-c2f2f440b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alyfa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "#!pip install datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "   text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "   text = re.sub(\"ى\", \"ي\", text)\n",
    "   text = re.sub(\"ؤ\", \"ء\", text)\n",
    "   text = re.sub(\"ئ\", \"ء\", text)\n",
    "   text = re.sub(\"ة\", \"ه\", text)\n",
    "   text = re.sub(\"گ\", \"ك\", text)\n",
    "   return text\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    return re.sub(arabic_diacritics, '', text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "    english_punctuations = string.punctuation\n",
    "    punctuations_list = arabic_punctuations + english_punctuations\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    word_list = nltk.tokenize.wordpunct_tokenize(text.lower())\n",
    "    word_list = [ w for w in word_list if not w in arabic_stop_words]\n",
    "    return (\" \".join(word_list)).strip()\n",
    "\n",
    "def remove_non_arabic_letters(text):\n",
    "    text = re.sub(r'([@A-Za-z0-9_]+)|#|http\\S+', ' ', text) # removes non arabic letters\n",
    "    text = re.sub(r'ـــــــــــــ', '', text) # removes non arabic letters\n",
    "    return text\n",
    "\n",
    "def clean_str(text):\n",
    "    text = normalize_arabic(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = remove_non_arabic_letters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee21a8c4-9a04-4629-b4b2-8feccf8c3d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f363755d7343acbb496fe6d21a625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2b075b6d3b455798b59d37d9ea1356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7b949b117e4f51a08542e10ed2cb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#preparting the data for training\n",
    "# Cleaning data\n",
    "train['cleaned_paragraph'] = train['paragraph'].apply(clean_str)\n",
    "dev['cleaned_paragraph'] = dev['paragraph'].apply(clean_str)\n",
    "test['cleaned_paragraph'] = test['paragraph'].apply(clean_str)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['cleaned_paragraph'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train[['cleaned_paragraph', 'binary_labels']])\n",
    "dev_dataset = Dataset.from_pandas(dev[['cleaned_paragraph', 'binary_labels']])\n",
    "test_dataset = Dataset.from_pandas(test[['cleaned_paragraph', 'binary_labels']])\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Rename columns to match the expected input format\n",
    "train_dataset = train_dataset.rename_column('binary_labels', 'labels')\n",
    "dev_dataset = dev_dataset.rename_column('binary_labels', 'labels')\n",
    "test_dataset = test_dataset.rename_column('binary_labels', 'labels')\n",
    "\n",
    "# Set format for PyTorch for multilabel classification\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dev_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d00c393-cfa2-457c-8db6-214589625a38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Model and Training arguments\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maubmindlab/bert-base-arabertv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_labels))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./multilabel_model\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Model and Training arguments\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"aubmindlab/bert-base-arabertv2\", num_labels=len(all_labels)).to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./multilabel_model',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = (p.predictions >= 0.5).astype(int)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Calculate precision, recall, f1 for both micro and macro\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro\n",
    "    }\n",
    "\n",
    "   \n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").to(device)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572b0dd-aa86-4720-b982-231957f5e9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
